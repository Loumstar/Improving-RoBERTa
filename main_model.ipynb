{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loumstar/Improving-RoBERTa/blob/master/main_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYyLA2fddXcB",
        "outputId": "e2248879-c1ce-4b65-e361-8441b0d97537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  3 14:21:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    30W /  70W |  10836MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-qgGacGc0iY"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTmK5tRp--Sh"
      },
      "source": [
        "## Installations & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scxgkTcbZK6j",
        "outputId": "f39821de-63f1-4c4c-d7fc-25868fe10bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.5)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (2.5)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.11)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece ipywidgets tensorboardx wandb\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "cG3IANnJZt9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87fe7cc-e7a8-493e-dbe0-fd88de55dfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "CUDA available.\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import string\n",
        "import wandb\n",
        "import nltk\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from urllib import request\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  print(\"CUDA available.\")\n",
        "  DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnHES8y--Si"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2eGdCwoFRCB",
        "outputId": "b458d114-1c35-4721-8fee-1f5fbf1d9fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "mkdir: cannot create directory ‘./nlp-cswrk’: File exists\n",
            "/content/drive/MyDrive/nlp-cswrk\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/'\n",
        "%mkdir './nlp-cswrk'\n",
        "%cd './nlp-cswrk' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHcqaICo--Sj"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "jnWXLw0cdbDs"
      },
      "outputs": [],
      "source": [
        "# prepare logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2_ed_gT--Sk"
      },
      "source": [
        "## Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4c_CmfHba0s",
        "outputId": "e58d0ba8-f05d-4d43-a3de-5db9ea552351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/train_semeval_parids-labels.csv\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/train_semeval_parids-labels.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii68DM91B4Ws",
        "outputId": "80f9776b-c993-4109-cc29-1ca149b40b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/dev_semeval_parids-labels.csv\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/dev_semeval_parids-labels.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNj1N-S9eMgi",
        "outputId": "624ce46d-ca59-4777-bbd7-74f532de39a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AZlPSk4adKOD"
      },
      "outputs": [],
      "source": [
        "from dont_patronize_me import DontPatronizeMe\n",
        "\n",
        "dpm = DontPatronizeMe('.', '.')\n",
        "dpm.load_task1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ArCxbwYdeQMZ"
      },
      "outputs": [],
      "source": [
        "# helper function to save predictions to an output file\n",
        "def labels2file(p, outf_path):\n",
        "\twith open(outf_path,'w') as outf:\n",
        "\t\tfor pi in p:\n",
        "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dxOesic5rz"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv9SL2thuP0d"
      },
      "source": [
        "## Data Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNMAMo48uv-5"
      },
      "source": [
        "### Convert CSVs to Data Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "roQDngVicMAl"
      },
      "outputs": [],
      "source": [
        "train_import = pd.read_csv('train_semeval_parids-labels.csv')\n",
        "test_import = pd.read_csv('dev_semeval_parids-labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "-gvAaCJjCu1S"
      },
      "outputs": [],
      "source": [
        "train_import.par_id = train_import.par_id.astype(str)\n",
        "test_import.par_id = test_import.par_id.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "IkYsxgeKclx2"
      },
      "outputs": [],
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(train_import)):\n",
        "  parid = train_import.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to retrieve `text` and binary label\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  intensity = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].orig_label.values[0]\n",
        "  length = len(dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0])\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label,\n",
        "      'keyword':keyword,\n",
        "      'country':country,\n",
        "      'intensity':intensity,\n",
        "      'length':length\n",
        "  })\n",
        "train_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "9hc81lCPC0i7"
      },
      "outputs": [],
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(test_import)):  \n",
        "  parid = test_import.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  intensity = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].orig_label.values[0]\n",
        "  length = len(dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0])\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label,\n",
        "      'keyword':keyword,\n",
        "      'country':country,\n",
        "      'intensity':intensity,\n",
        "      'length':length\n",
        "  })\n",
        "  test_df = pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTL-8BcouvHi"
      },
      "source": [
        "### Shuffle Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "T5aDrxnK47n2"
      },
      "outputs": [],
      "source": [
        "train_df = shuffle(train_df)\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, train_size=0.8)\n",
        "\n",
        "test_df = shuffle(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_A9nN46vVnb"
      },
      "source": [
        "## Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSyDTq41nbD"
      },
      "source": [
        "### Main Function\n",
        "Enter iteration number to pre-process data corresponding to the implementation iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bHQNcXNUwCQA"
      },
      "outputs": [],
      "source": [
        "def main_implement(implementation_number, train):\n",
        "    if implementation_number == 1:\n",
        "        train = downsample_unpat(train)\n",
        "    elif implementation_number == 2:\n",
        "        train = downsample_unpat(train)\n",
        "        train = upsample_pat(train)\n",
        "    elif implementation_number == 3:\n",
        "        train = downsample_unpat(train)\n",
        "        train = lemmatise_df(train)\n",
        "    elif implementation_number == 4:\n",
        "        train = add_backtranslations(train)\n",
        "        train = downsample_unpat(train)\n",
        "    train.length = train.text.str.split().map(get_list_length)\n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list_length(x):\n",
        "  if type(x) == list:\n",
        "    return len(x)\n",
        "  elif x is not None:\n",
        "    return 1\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "u4GkRtyTE5Sq"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "fGS2OpFwvUNU"
      },
      "outputs": [],
      "source": [
        "def get_label_ratio(dataset):\n",
        "    return len(dataset[dataset.label == 0])/len(dataset[dataset.label == 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXzdxQdvbGW"
      },
      "source": [
        "### Implementation 1\n",
        "Downsample unpatronising data (as in competition's roberta base) (ratio from 9:1 to 3:1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "bxxYlfioHBdp"
      },
      "outputs": [],
      "source": [
        "def downsample_unpat(train):\n",
        "    pat_train = train[train.label == 1]\n",
        "    npos = len(pat_train)\n",
        "\n",
        "    train = pd.concat([pat_train,train[train.label == 0][:npos*3]])\n",
        "    return train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aZXFKfKylBj"
      },
      "source": [
        "### Implementation 2\n",
        "Upsample patronising data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "ypnQZgAkyJSz"
      },
      "outputs": [],
      "source": [
        "def upsample_pat(train):\n",
        "    ratio = int(get_label_ratio(train))\n",
        "\n",
        "    pat_train = train[train.label == 1]\n",
        "    unpat_train = train[train.label == 0]\n",
        "\n",
        "    pat_train = pd.concat([pat_train]*ratio)\n",
        "    train = pd.concat([pat_train, unpat_train])\n",
        "    \n",
        "    return train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xN3crFMPLYU6"
      },
      "outputs": [],
      "source": [
        "def df_to_dict_of_lists(df):\n",
        "    texts = df['text'].values.tolist()\n",
        "    labels = df['label'].values.tolist()\n",
        "    parids = df['par_id'].values.tolist()\n",
        "    keywords = df['keyword'].values.tolist()\n",
        "    countries = df['country'].values.tolist()\n",
        "    intensities = df['intensity'].values.tolist()\n",
        "    lengths = df['length'].values.tolist()\n",
        "    \n",
        "    return {\n",
        "        'texts': texts,\n",
        "        'labels': labels,\n",
        "        'parids': parids,\n",
        "        'keywords': keywords,\n",
        "        'countries': countries,\n",
        "        'intensities': intensities,\n",
        "        'lengths': lengths\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XZxdZsRZF8X"
      },
      "source": [
        "### Implementation 3\n",
        "Lemmatise words in text using verb PoS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "vwSsf79Ek7Pt"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_punctuation(token):\n",
        "    return not any(map(lambda s: s in string.punctuation, token))\n",
        "\n",
        "def lemmatise(word):\n",
        "    return lemmatizer.lemmatize(word, pos=\"v\")\n",
        "\n",
        "def lemmatise_sentence(sentence):\n",
        "  tokens = wordpunct_tokenize(sentence)\n",
        "  tokens = list(map(lemmatise, tokens))\n",
        "  # tokens = list(filter(remove_punctuation, tokens))\n",
        "\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "def lemmatise_df(df):\n",
        "  df.text = df.text.map(lemmatise_sentence)\n",
        "  \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvmASpcpc5J"
      },
      "source": [
        "### Implementation 4\n",
        "Generate new examples using back-translation (of French and Spanish)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9PFXT7rBqzHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1044a96-2295-41df-9484-d9406dc8793b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_french_backtranslation.csv\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_french_backtranslation.csv\" \n",
        "module_name = module_url.split('/')[-1]\n",
        "\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('latin-1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "SeugFi36Y48R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c639cd-7f0a-44b6-d496-3337d8f16405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_spanish_backtranslation.csv\n"
          ]
        }
      ],
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_spanish_backtranslation.csv\" \n",
        "module_name = module_url.split('/')[-1]\n",
        "\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('latin-1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "OqI821zTtLQl"
      },
      "outputs": [],
      "source": [
        "backtranslated_cols = [\"par_id\", \"art_id\", \"category\", \"country\", \"text\", \"score\"]\n",
        "\n",
        "french_import = pd.read_csv('dontpatronizeme_french_backtranslation.csv', names=backtranslated_cols)\n",
        "spanish_import = pd.read_csv('dontpatronizeme_spanish_backtranslation.csv', names=backtranslated_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "LJk7f-IItjb7"
      },
      "outputs": [],
      "source": [
        "# Copy the training dataset and swap out the text \n",
        "# for the back-translated version.\n",
        "french_df = train_df.copy()\n",
        "french_df.text = french_import.text\n",
        "\n",
        "spanish_df = train_df.copy()\n",
        "spanish_df.text = spanish_import.text\n",
        "\n",
        "def add_backtranslations(df):\n",
        "  df = df.append(french_df, ignore_index=True, \n",
        "                 verify_integrity=True)\n",
        "  \n",
        "  df = df.append(spanish_df, ignore_index=True, \n",
        "                 verify_integrity=True)\n",
        "  \n",
        "  df.length = df.text.str.split().map(get_list_length)\n",
        "  df.dropna(inplace=True)\n",
        "  \n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnNmRnPxBUl1"
      },
      "source": [
        "### Choose Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "fYK_2Qy2BXqc"
      },
      "outputs": [],
      "source": [
        "train_df = main_implement(3, train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "qVsAwo7rSHKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1c52d6-c122-4022-9b97-7fe7b88072bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n"
          ]
        }
      ],
      "source": [
        "print(get_label_ratio(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-Z90mgCzEWxO",
        "outputId": "2e1d22aa-c205-410a-8a0b-08c5f0e36b58"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-35bda447-7365-4399-a247-02b4441afd77\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>par_id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>keyword</th>\n",
              "      <th>country</th>\n",
              "      <th>intensity</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>8340</td>\n",
              "      <td>Sr Luise earn the trust of development partner...</td>\n",
              "      <td>1</td>\n",
              "      <td>refugee</td>\n",
              "      <td>ke</td>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>1273</td>\n",
              "      <td>The come fury of angry America be as palpable ...</td>\n",
              "      <td>1</td>\n",
              "      <td>hopeless</td>\n",
              "      <td>in</td>\n",
              "      <td>3</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>3357</td>\n",
              "      <td>The former Jewish neighbourhood in the city of...</td>\n",
              "      <td>1</td>\n",
              "      <td>poor-families</td>\n",
              "      <td>gb</td>\n",
              "      <td>3</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6027</td>\n",
              "      <td>As a member of Care of Hope , Barnett provide ...</td>\n",
              "      <td>1</td>\n",
              "      <td>homeless</td>\n",
              "      <td>jm</td>\n",
              "      <td>4</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>6552</td>\n",
              "      <td>\"-- In 2014 Make the Future showcased Pavegen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>in-need</td>\n",
              "      <td>ng</td>\n",
              "      <td>2</td>\n",
              "      <td>121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3830</th>\n",
              "      <td>3369</td>\n",
              "      <td>A team of experts across several discipline at...</td>\n",
              "      <td>0</td>\n",
              "      <td>women</td>\n",
              "      <td>ph</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2625</th>\n",
              "      <td>2046</td>\n",
              "      <td>San Francisco be currently face a hard time wi...</td>\n",
              "      <td>0</td>\n",
              "      <td>homeless</td>\n",
              "      <td>lk</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3949</th>\n",
              "      <td>3501</td>\n",
              "      <td>\"\"\" My parent use to say , always help people ...</td>\n",
              "      <td>0</td>\n",
              "      <td>in-need</td>\n",
              "      <td>ph</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2907</th>\n",
              "      <td>2353</td>\n",
              "      <td>Market confidence be a hopelessly fragile sens...</td>\n",
              "      <td>0</td>\n",
              "      <td>hopeless</td>\n",
              "      <td>au</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6910</th>\n",
              "      <td>6769</td>\n",
              "      <td>The winner of the Miss Handicap Cameroon 2018 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>disabled</td>\n",
              "      <td>ke</td>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2532 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35bda447-7365-4399-a247-02b4441afd77')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35bda447-7365-4399-a247-02b4441afd77 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35bda447-7365-4399-a247-02b4441afd77');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     par_id  ... length\n",
              "501    8340  ...     50\n",
              "342    1273  ...     57\n",
              "532    3357  ...     47\n",
              "6      6027  ...     59\n",
              "786    6552  ...    121\n",
              "...     ...  ...    ...\n",
              "3830   3369  ...    100\n",
              "2625   2046  ...     43\n",
              "3949   3501  ...     45\n",
              "2907   2353  ...     44\n",
              "6910   6769  ...     85\n",
              "\n",
              "[2532 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arGrcVHI-0cd"
      },
      "source": [
        "## More Data Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL3uUoHQ-4UV"
      },
      "source": [
        "Convert data to class and create embeddings from tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "rV1muNSZNHd3"
      },
      "outputs": [],
      "source": [
        "class PatroDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_set, max_length=128):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        self.parids = input_set['parids']\n",
        "        self.keywords = input_set['keywords']\n",
        "        self.countries = input_set['countries']\n",
        "        self.intensities = input_set['intensities']\n",
        "        self.lengths = input_set['lengths']\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        texts, labels = [], []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b['text'])\n",
        "            labels.append(b['label'])\n",
        "\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', \n",
        "                                   padding=True, truncation=True, \n",
        "                                   max_length=self.max_length)\n",
        "        \n",
        "        encodings['label'] =  torch.tensor(labels)\n",
        "        \n",
        "        return encodings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       \n",
        "        return {\n",
        "            'text': self.texts[idx],\n",
        "            'label': self.labels[idx],\n",
        "            'parid': self.parids[idx],\n",
        "            'keyword': self.keywords[idx],\n",
        "            'countriy': self.countries[idx],\n",
        "            'intensity': self.intensities[idx],\n",
        "            'length': self.lengths[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "eWYbesCMN7Mk"
      },
      "outputs": [],
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "uRynl_q2N9R5"
      },
      "outputs": [],
      "source": [
        "trainset = df_to_dict_of_lists(train_df)\n",
        "valset = df_to_dict_of_lists(val_df)\n",
        "testset = df_to_dict_of_lists(test_df)\n",
        "\n",
        "val_dataset = PatroDataset(tokenizer, testset)\n",
        "test_dataset = PatroDataset(tokenizer, testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHWXEdWnF9Ne"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXweseRdq6D_"
      },
      "source": [
        "##### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "N633yUziq84U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e320ae2-e084-4a03-a4a0-0017f049079b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: lpokxkkx\n",
            "Sweep URL: https://wandb.ai/matthewcollins/DeBERTa%20Hyperparameter%20Tuning/sweeps/lpokxkkx\n"
          ]
        }
      ],
      "source": [
        "hyperparameter_tuning = True\n",
        "\n",
        "if hyperparameter_tuning:\n",
        "    wandb.login()\n",
        "\n",
        "    config = {\n",
        "        \"name\": \"vanilla-sweep\",\n",
        "        \"method\": \"grid\",\n",
        "        \"metric\": {\n",
        "            \"name\": \"f1_score\",\n",
        "            \"goal\": \"maximize\"\n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"num_epochs\": {\n",
        "                \"values\":[3, 4, 5]\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"values\":[1e-4, 1.5e-4, 2e-4]\n",
        "            },\n",
        "            \"max_length\": {\n",
        "                \"values\": [64, 128, 256]\n",
        "            }\n",
        "        },\n",
        "        \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 3},\n",
        "    }\n",
        "\n",
        "    sweep_id = wandb.sweep(config, project=\"DeBERTa Hyperparameter Tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJsaOyISAq5u"
      },
      "source": [
        "DeBERTa Implementation and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "UOawO4NpGCpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ebbaf2-502a-474f-e8bb-79fe8eafe803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 139193858\n"
          ]
        }
      ],
      "source": [
        "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "#180 M\n",
        "print(f\"Model size: {model.num_parameters()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "m3a-PN-PLAO_"
      },
      "outputs": [],
      "source": [
        "class Trainer_patronise(Trainer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.losses = []\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = inputs.pop('label')\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_task = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = loss_task(logits, labels)\n",
        "        self.losses.append(loss.item())\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "WuJkwU8WLEgl"
      },
      "outputs": [],
      "source": [
        "def main_patronise():\n",
        "    if hyperparameter_tuning:\n",
        "        wandb.init()\n",
        "        learning_rate = wandb.config.learning_rate\n",
        "        max_length = wandb.config.max_length\n",
        "        num_epochs = wandb.config.num_epochs\n",
        "    else:\n",
        "        learning_rate = 0.0001\n",
        "        max_length = 128\n",
        "        num_epochs = 4\n",
        "\n",
        "    train_dataset = PatroDataset(tokenizer, trainset,\n",
        "                                 max_length=max_length)\n",
        "    \n",
        "    val_loader = DataLoader(val_dataset)\n",
        "\n",
        "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
        "    model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./',\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler_type='linear',\n",
        "        logging_steps=100,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs=num_epochs,\n",
        "        adam_beta1 = 0.9,\n",
        "        adam_beta2 = 0.999,\n",
        "        adam_epsilon = 1e-6,\n",
        "        max_grad_norm = 1.0,\n",
        "        save_steps = 2500\n",
        "    )\n",
        "    trainer = Trainer_patronise(\n",
        "        model=model,              \n",
        "        args=training_args,                \n",
        "        train_dataset=train_dataset,          \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "    trainer.save_model('./')\n",
        "\n",
        "    report, _ = evaluate(model, tokenizer, val_loader, val_df)\n",
        "    f1_score = report[1][\"f1-score\"]\n",
        "\n",
        "    if hyperparameter_tuning:\n",
        "        wandb.log({\"f1_score\": f1_score})  \n",
        "        wandb.join()\n",
        "\n",
        "    return trainer.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srfNerRdgTSH"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameter Training"
      ],
      "metadata": {
        "id": "9M8uC_y1MUtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "01ivcIlHgXTG"
      },
      "outputs": [],
      "source": [
        "def predict_patronise(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  output = model(**encodings)\n",
        "  preds = torch.max(output.logits, 1)\n",
        "\n",
        "  return {'prediction':preds[1], 'confidence':preds[0]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "98BAJTSygeLW"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer, data_loader, test_df):\n",
        "\n",
        "  total_count = 0\n",
        "  correct_count = 0 \n",
        "\n",
        "  preds = []\n",
        "  tot_labels = []\n",
        "\n",
        "  test_df['prediction'] = np.nan\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader):\n",
        "      labels = {}\n",
        "      labels['label'] = data['label']\n",
        "\n",
        "      sentences = data['text']\n",
        "\n",
        "      pred = predict_patronise(sentences, tokenizer, model)\n",
        "\n",
        "      preds.append(pred['prediction'].tolist())\n",
        "      tot_labels.append(labels['label'].tolist())\n",
        "\n",
        "      for id, data_prediction in zip(data['parid'], pred['prediction']):\n",
        "        rows = test_df.loc[test_df.par_id == id]\n",
        "        rows.prediction = data_prediction.tolist()\n",
        "        test_df.update(rows)\n",
        "\n",
        "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
        "  report = classification_report(tot_labels, preds, target_names=[0,1], output_dict= True)\n",
        "\n",
        "  return report, test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if hyperparameter_tuning:\n",
        "    losses = wandb.agent(sweep_id, function=main_patronise, count=10)\n",
        "else:\n",
        "    losses = main_patronise()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j430z_OKMbvM",
        "outputId": "9b39757f-f06d-4a0e-813f-36a3ec3c8925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yf47dqvs with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/nlp-cswrk/wandb/run-20220303_143200-yf47dqvs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/matthewcollins/DeBERTa%20Hyperparameter%20Tuning/runs/yf47dqvs\" target=\"_blank\">warm-sweep-3</a></strong> to <a href=\"https://wandb.ai/matthewcollins/DeBERTa%20Hyperparameter%20Tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/matthewcollins/DeBERTa%20Hyperparameter%20Tuning/sweeps/lpokxkkx\" target=\"_blank\">https://wandb.ai/matthewcollins/DeBERTa%20Hyperparameter%20Tuning/sweeps/lpokxkkx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/microsoft/deberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
            "Model config DebertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": [\n",
            "    \"c2p\",\n",
            "    \"p2c\"\n",
            "  ],\n",
            "  \"position_biased_input\": false,\n",
            "  \"relative_attention\": true,\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/deberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dde0725208c11536042f6f416c538792d44a2d57d1ae399bbd1bc5867e02c465.0a3ec262cb3d4f634c72ce55f2766bb88771e6499b2512830e2e63bf19dbf97a\n",
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 2532\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 400\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_length' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [114/400 00:50 < 02:09, 2.20 it/s, Epoch 1.41/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.484800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model"
      ],
      "metadata": {
        "id": "QOvGCWmQMeP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE HYPER-PARAMETERS AND RUN MODEL ON WHOLE TRAINING SET"
      ],
      "metadata": {
        "id": "04lOgycWMknn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "YgOHAGRlMoMN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "1a2624dd-abd1-46b3-ef25-0b4d01bc140b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-1f71c63ba2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# element array of None which causes problems downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x, y, and format string must not be None"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvhnJKkdZoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z9aCSpPWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WlU22NI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuM4fcJEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZcum6w2goAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2cBAJ3WrI0g"
      },
      "outputs": [],
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './'\n",
        "model = DebertaForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# we don't batch our test set unless it's too big\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report, annotated_test_dataset = evaluate(model, tokenizer, test_loader, test_df)\n",
        "\n",
        "print(report)\n",
        "\n",
        "print(report['accuracy'])\n",
        "print(report[0]['f1-score'])\n",
        "print(report[1]['f1-score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYoGSGZn4LTC"
      },
      "outputs": [],
      "source": [
        "annotated_test_dataset[annotated_test_dataset.prediction==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H2KXutVbGdM"
      },
      "outputs": [],
      "source": [
        "def analysis_on_intensities(annotated_dataset):\n",
        "    scores = []\n",
        "    for intensity_value in range(0,5):\n",
        "        subset_dataset = annotated_dataset.loc[annotated_dataset.intensity.map(int) == intensity_value]\n",
        "        labels = subset_dataset['label'].tolist()\n",
        "        preds = subset_dataset['prediction'].tolist()\n",
        "        score = f1_score(labels, preds)\n",
        "        print(score)\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKPqeZWubXHA"
      },
      "outputs": [],
      "source": [
        "analysis_on_intensities(annotated_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39M3nT_tgY0D"
      },
      "outputs": [],
      "source": [
        "def analysis_on_input_size(annotated_dataset):\n",
        "    for i in range(0,6):\n",
        "        subset_dataset = annotated_dataset.loc[i*50 <= annotated_dataset.length.map(int) < (i+1)*50]\n",
        "        \n",
        "\n",
        "\n",
        "    plt.title(\"Average Character Length of a sample\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xlabel(\"Character Length\")\n",
        "\n",
        "    # series = df[\"text\"].str.len()\n",
        "\n",
        "    # print(series.max())\n",
        "    # series.hist(bins=200, figsize=(8, 6))\n",
        "\n",
        "    # unpatronising_df[\"text\"].str.len().hist(bins=200, alpha=0.5, figsize=(8, 6))\n",
        "    # patronising_df[\"text\"].str.len().hist(bins=200, alpha=0.5, figsize=(8, 6))\n",
        "\n",
        "    (unpatcounts, unpatbins) = np.histogram(unpatronising_df[\"text\"].str.len(), bins=100, range=(0, 2000))\n",
        "    (patcounts, patbins) = np.histogram(patronising_df[\"text\"].str.len(), bins=100, range=(0, 2000))\n",
        "\n",
        "    factor = 1/len(unpatronising_df[\"text\"])\n",
        "    plt.hist(unpatbins[:-1], unpatbins, weights=factor*unpatcounts, alpha=0.6)\n",
        "\n",
        "    factor = 1/len(patronising_df[\"text\"])\n",
        "    plt.hist(patbins[:-1], patbins, weights=factor*patcounts, alpha=0.6)\n",
        "\n",
        "    plt.xlim((0, 2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD-2oo9OwSuH"
      },
      "outputs": [],
      "source": [
        "print(report['accuracy'])\n",
        "print(report[0]['f1-score'])\n",
        "print(report[1]['f1-score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXGXYABh0aXo"
      },
      "outputs": [],
      "source": [
        "\n",
        "report = classification_report(tot_labels, preds, target_names=[0,1], output_dict= True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "main_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}