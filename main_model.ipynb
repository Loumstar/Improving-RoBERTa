{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loumstar/Improving-RoBERTa/blob/master/main_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALLATIONS\n"
      ],
      "metadata": {
        "id": "q-qgGacGc0iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYyLA2fddXcB",
        "outputId": "99d46e2b-13af-45e8-b436-bec6c1fa6382"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  3 12:48:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "scxgkTcbZK6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9188fdb-7146-4f78-c5c0-61aa78d3515a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (4.10.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.11.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.7)\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.11)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install tensorboardx\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "from transformers import DebertaTokenizer, DebertaPreTrainedModel, DebertaForSequenceClassification\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ],
      "metadata": {
        "id": "cG3IANnJZt9T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "import logging\n",
        "from collections import Counter\n",
        "from ast import literal_eval"
      ],
      "metadata": {
        "id": "PVZ7M2G-bz3Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/'\n",
        "%mkdir './nlp-cswrk'\n",
        "%cd './nlp-cswrk' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2eGdCwoFRCB",
        "outputId": "d054dac9-5030-4a4f-82f7-2ecc1fa3c047"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "mkdir: cannot create directory ‘./nlp-cswrk’: File exists\n",
            "/content/drive/MyDrive/nlp-cswrk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare logger\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# check gpu\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "print('Cuda available? ',cuda_available)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnWXLw0cdbDs",
        "outputId": "bceac626-3581-42df-81aa-ebde884f464d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/train_semeval_parids-labels.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4c_CmfHba0s",
        "outputId": "a69a1f55-47cb-4a49-de1b-19e10edca906"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/train_semeval_parids-labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/dev_semeval_parids-labels.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii68DM91B4Ws",
        "outputId": "63840c34-d7dc-4477-8c44-bc58daaee190"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/practice%20splits/dev_semeval_parids-labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNj1N-S9eMgi",
        "outputId": "9297383a-3006-4ca9-b93d-b3b536cf32ea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dont_patronize_me import DontPatronizeMe"
      ],
      "metadata": {
        "id": "CuqymizEcyq4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "dpm = DontPatronizeMe('.', '.')\n",
        "dpm.load_task1()"
      ],
      "metadata": {
        "id": "AZlPSk4adKOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce58f60b-7ca1-4327-d11e-f72c74404ad5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/nlp-cswrk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to save predictions to an output file\n",
        "def labels2file(p, outf_path):\n",
        "\twith open(outf_path,'w') as outf:\n",
        "\t\tfor pi in p:\n",
        "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
      ],
      "metadata": {
        "id": "ArCxbwYdeQMZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRE-PROCESSING"
      ],
      "metadata": {
        "id": "a1dxOesic5rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Formatting"
      ],
      "metadata": {
        "id": "Vv9SL2thuP0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert csv files into dataframes"
      ],
      "metadata": {
        "id": "uNMAMo48uv-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_import = pd.read_csv('train_semeval_parids-labels.csv')\n",
        "test_import = pd.read_csv('dev_semeval_parids-labels.csv')"
      ],
      "metadata": {
        "id": "roQDngVicMAl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_import.par_id = train_import.par_id.astype(str)\n",
        "test_import.par_id = test_import.par_id.astype(str)"
      ],
      "metadata": {
        "id": "-gvAaCJjCu1S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(train_import)):\n",
        "  parid = train_import.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to retrieve `text` and binary label\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  intensity = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].orig_label.values[0]\n",
        "  length = len(dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0])\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label,\n",
        "      'keyword':keyword,\n",
        "      'country':country,\n",
        "      'intensity':intensity,\n",
        "      'length':length\n",
        "  })\n",
        "train_df = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "IkYsxgeKclx2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(test_import)):  \n",
        "  parid = test_import.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  intensity = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].orig_label.values[0]\n",
        "  length = len(dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0])\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      'text':text,\n",
        "      'label':label,\n",
        "      'keyword':keyword,\n",
        "      'country':country,\n",
        "      'intensity':intensity,\n",
        "      'length':length\n",
        "  })\n",
        "  test_df = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "9hc81lCPC0i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle data"
      ],
      "metadata": {
        "id": "JTL-8BcouvHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = shuffle(train_df)"
      ],
      "metadata": {
        "id": "T5aDrxnK47n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = shuffle(test_df)"
      ],
      "metadata": {
        "id": "41sPCgG63Y7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementations"
      ],
      "metadata": {
        "id": "8_A9nN46vVnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main functions: Enter iteration number to pre-process data corresponding to the implementation iteration."
      ],
      "metadata": {
        "id": "clSyDTq41nbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_implement(iteration_number, train):\n",
        "    if iteration_number == 1:\n",
        "        train = downsample_unpat(train)\n",
        "    elif iteration_number == 2:\n",
        "        train = downsample_unpat(train)\n",
        "        train = upsample_pat(train)\n",
        "    elif iteration_number == 3:\n",
        "        train = downsample_unpat(train)\n",
        "        train = lemmatise_df(train)\n",
        "    elif iteration_number == 4:\n",
        "        train = add_backtranslations(train)\n",
        "        train = downsample_unpat(train)\n",
        "    \n",
        "    return train"
      ],
      "metadata": {
        "id": "bHQNcXNUwCQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_ratio(dataset):\n",
        "    return len(dataset[dataset.label == 0])/len(dataset[dataset.label == 1])"
      ],
      "metadata": {
        "id": "fGS2OpFwvUNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation 1: Downsample unpatronising data (as in competition's roberta base) (ratio from 9:1 to 3:1)."
      ],
      "metadata": {
        "id": "QuXzdxQdvbGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_unpat(train):\n",
        "    pat_train = train[train.label == 1]\n",
        "    npos = len(pat_train)\n",
        "\n",
        "    train = pd.concat([pat_train,train[train.label == 0][:npos*3]])\n",
        "    return train"
      ],
      "metadata": {
        "id": "bxxYlfioHBdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation 2: Upsample patronising data"
      ],
      "metadata": {
        "id": "0aZXFKfKylBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample_pat(train):\n",
        "    ratio = int(get_label_ratio(train))\n",
        "\n",
        "    pat_train = train[train.label == 1]\n",
        "    unpat_train = train[train.label == 0]\n",
        "\n",
        "    pat_train = pd.concat([pat_train]*ratio)\n",
        "    train = pd.concat([pat_train, unpat_train])\n",
        "    return train\n"
      ],
      "metadata": {
        "id": "ypnQZgAkyJSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dict_of_lists(df):\n",
        "    texts = df['text'].values.tolist()\n",
        "    labels = df['label'].values.tolist()\n",
        "    parids = df['par_id'].values.tolist()\n",
        "    keywords = df['keyword'].values.tolist()\n",
        "    countries = df['country'].values.tolist()\n",
        "    intensities = df['intensity'].values.tolist()\n",
        "    lengths = df['length'].values.tolist()\n",
        "    return {'texts':texts,\n",
        "            'labels':labels,\n",
        "            'parids':parids,\n",
        "            'keywords':keywords,\n",
        "            'countries':countries,\n",
        "            'intensities':intensities,\n",
        "            'lengths':lengths}"
      ],
      "metadata": {
        "id": "xN3crFMPLYU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation 3: Lemmatisation"
      ],
      "metadata": {
        "id": "7XZxdZsRZF8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_stop_words(token):\n",
        "    return token not in stopwords.words(\"english\")\n",
        "\n",
        "def remove_punctuation(token):\n",
        "    return not any(map(lambda s: s in string.punctuation, token))\n",
        "\n",
        "def lemmatise(word):\n",
        "    return lemmatizer.lemmatize(word, pos=\"v\")\n",
        "\n",
        "def lemmatise_text(sentence):\n",
        "  tokens = wordpunct_tokenize(sentence)\n",
        "  tokens = list(map(lemmatise, tokens))\n",
        "\n",
        "  return \" \".join(tokens)\n",
        "  \n",
        "  # tokens = filter(remove_punctuation, tokens)\n",
        "  \"\"\"\n",
        "  tokens = filter(remove_stop_words, tokens)\n",
        "  \"\"\"\n",
        "\n",
        "def lemmatise_df(df):\n",
        "  df.text.update(df.text.map(lemmatise_text))\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "vwSsf79Ek7Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation 4: Back-translation\n",
        "\n"
      ],
      "metadata": {
        "id": "pSvmASpcpc5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_french_backtranslation.csv\" \n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('latin-1'))"
      ],
      "metadata": {
        "id": "9PFXT7rBqzHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Loumstar/Improving-RoBERTa/master/data/dontpatronizeme_spanish_backtranslation.csv\" \n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  b = f.read()\n",
        "  outf.write(b.decode('latin-1'))"
      ],
      "metadata": {
        "id": "SeugFi36Y48R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backtranslated_cols = [\"par_id\", \"art_id\", \"category\", \"country\", \"text\", \"score\"]\n",
        "\n",
        "french_import = pd.read_csv('dontpatronizeme_french_backtranslation.csv', names=backtranslated_cols)\n",
        "spanish_import = pd.read_csv('dontpatronizeme_spanish_backtranslation.csv', names=backtranslated_cols)"
      ],
      "metadata": {
        "id": "OqI821zTtLQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list_length(x):\n",
        "  if type(x) == list:\n",
        "    return len(x)\n",
        "  elif type(x) != None:\n",
        "    return 1\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "french_df = train_df.copy()\n",
        "spanish_df = train_df.copy()\n",
        "\n",
        "french_df.text = french_import.text\n",
        "spanish_df.text = spanish_import.text\n",
        "\n",
        "def add_backtranslations(df):\n",
        "  df = df.append(french_df, ignore_index=True, \n",
        "                 verify_integrity=True)\n",
        "  \n",
        "  df = df.append(spanish_df, ignore_index=True, \n",
        "                 verify_integrity=True)\n",
        "  \n",
        "  df.length = df.text.str.split().map(get_list_length)\n",
        "  df.dropna(inplace=True)\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "LJk7f-IItjb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(french_df.text.head())\n",
        "print(spanish_df.text.head())\n",
        "\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "VUopvtOJvOgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHOOSE IMPLEMENTATION"
      ],
      "metadata": {
        "id": "cnNmRnPxBUl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = main_implement(3, train_df)"
      ],
      "metadata": {
        "id": "fYK_2Qy2BXqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_label_ratio(train_df))"
      ],
      "metadata": {
        "id": "qVsAwo7rSHKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Data Formatting"
      ],
      "metadata": {
        "id": "arGrcVHI-0cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert data to class and create embeddings from tokeniser"
      ],
      "metadata": {
        "id": "pL3uUoHQ-4UV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatroDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_set, max_length=128):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        self.parids = input_set['parids']\n",
        "        self.keywords = input_set['keywords']\n",
        "        self.countries = input_set['countries']\n",
        "        self.intensities = input_set['intensities']\n",
        "        self.lengths = input_set['lengths']\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        texts = []\n",
        "        labels = []\n",
        "        # parids = []\n",
        "        # keywords = []\n",
        "        # countries = []\n",
        "        # intensities = []\n",
        "        # lengths = []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b['text'])\n",
        "            labels.append(b['label'])\n",
        "            # parids.append(b['parids'])\n",
        "            # keywords.append(b['keywords'])\n",
        "            # countries.append(b['countries'])\n",
        "            # intensities.append(b['intensities'])\n",
        "            # lengths.append(b['lengths'])\n",
        "\n",
        "\n",
        "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
        "        # We also pad shorter sentences to a length of 128 tokens\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True,\n",
        "                                   truncation=True, max_length=self.max_length)\n",
        "        \n",
        "        # labels = {}\n",
        "        encodings['label'] =  torch.tensor(labels)\n",
        "        \n",
        "        return encodings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       \n",
        "        item = {'text': self.texts[idx],\n",
        "                'label': self.labels[idx],\n",
        "                'parid': self.parids[idx],\n",
        "                'keyword': self.keywords[idx],\n",
        "                'countriy': self.countries[idx],\n",
        "                'intensity': self.intensities[idx],\n",
        "                'length': self.lengths[idx]\n",
        "                }\n",
        "        return item"
      ],
      "metadata": {
        "id": "rV1muNSZNHd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")"
      ],
      "metadata": {
        "id": "eWYbesCMN7Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = df_to_dict_of_lists(train_df)\n",
        "testset = df_to_dict_of_lists(test_df)\n",
        "\n",
        "test_dataset = PatroDataset(tokenizer, testset)"
      ],
      "metadata": {
        "id": "uRynl_q2N9R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "RHWXEdWnF9Ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ZXweseRdq6D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import logging\n",
        "\n",
        "hyperparameter_tuning = False\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "config = {\n",
        "    \"name\": \"vanilla-sweep\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"f1_score\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"num_epochs\": {\"min\": 1, \"max\": 10},\n",
        "        \"learning_rate\": {\n",
        "            \"values\":[1e-4, 1.5e-4, 2e-4]\n",
        "        },\n",
        "        \"max_lengths\": {\n",
        "            \"values\": [64, 128, 256]\n",
        "        }\n",
        "    },\n",
        "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 3},\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(config, project=\"DeBERTa Hyperparameter Tuning\")"
      ],
      "metadata": {
        "id": "N633yUziq84U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeBERTa Implementation and Training"
      ],
      "metadata": {
        "id": "xJsaOyISAq5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "#180 M\n",
        "print(f\"Model size: {model.num_parameters()}\")"
      ],
      "metadata": {
        "id": "UOawO4NpGCpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer_patronise(Trainer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.losses = []\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = inputs.pop('label')\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_task = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = loss_task(logits, labels)\n",
        "        self.losses.append(loss.item())\n",
        "        \n",
        "        return loss"
      ],
      "metadata": {
        "id": "m3a-PN-PLAO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_patronise():\n",
        "    if hyperparameter_tuning:\n",
        "        wandb.init()\n",
        "        learning_rate = wandb.config.learning_rate\n",
        "        max_length = wandb.config.max_length\n",
        "        num_epochs = wandb.config.num_epochs\n",
        "    else:\n",
        "        learning_rate = 0.0001\n",
        "        max_length = 128\n",
        "        num_epochs = 4\n",
        "\n",
        "    train_dataset = PatroDataset(tokenizer, trainset,\n",
        "                                 max_length=max_length)\n",
        "\n",
        "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
        "    model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./',\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler_type='linear',\n",
        "        logging_steps=100,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs=num_epochs,\n",
        "        adam_beta1 = 0.9,\n",
        "        adam_beta2 = 0.999,\n",
        "        adam_epsilon = 1e-6,\n",
        "        max_grad_norm = 1.0,\n",
        "        save_steps = 2500\n",
        "    )\n",
        "    trainer = Trainer_patronise(\n",
        "        model=model,              \n",
        "        args=training_args,                \n",
        "        train_dataset=train_dataset,          \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    \n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./')\n",
        "\n",
        "    if hyperparameter_tuning:\n",
        "      wandb.log({\"f1_score\": f1})  \n",
        "      wandb.join()\n",
        "\n",
        "    return trainer.losses"
      ],
      "metadata": {
        "id": "WuJkwU8WLEgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if hyperparameter_tuning:\n",
        "    losses = wandb.agent(sweep_id, function=main_patronize, count=10)\n",
        "else:\n",
        "    losses = main_patronize()"
      ],
      "metadata": {
        "id": "JKvunpdJgLO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "srfNerRdgTSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "YgOHAGRlMoMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_patronise(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  output = model(**encodings)\n",
        "  preds = torch.max(output.logits, 1)\n",
        "\n",
        "  return {'prediction':preds[1], 'confidence':preds[0]}"
      ],
      "metadata": {
        "id": "01ivcIlHgXTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, tokenizer, data_loader, test_df):\n",
        "\n",
        "  total_count = 0\n",
        "  correct_count = 0 \n",
        "\n",
        "  preds = []\n",
        "  tot_labels = []\n",
        "\n",
        "  test_df['prediction'] = np.nan\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader):\n",
        "      labels = {}\n",
        "      labels['label'] = data['label']\n",
        "\n",
        "      sentences = data['text']\n",
        "\n",
        "      pred = predict_patronise(sentences, tokenizer, model)\n",
        "\n",
        "      preds.append(pred['prediction'].tolist())\n",
        "      tot_labels.append(labels['label'].tolist())\n",
        "\n",
        "      for id, data_prediction in zip(data['parid'], pred['prediction']):\n",
        "        rows = test_df.loc[test_df.par_id == id]\n",
        "        rows.prediction = data_prediction.tolist()\n",
        "        test_df.update(rows)\n",
        "\n",
        "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
        "  report = classification_report(tot_labels, preds, target_names=[0,1], output_dict= True)\n",
        "\n",
        "  return report, test_df"
      ],
      "metadata": {
        "id": "98BAJTSygeLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './'\n",
        "model = DebertaForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# we don't batch our test set unless it's too big\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report, annotated_test_dataset = evaluate(model, tokenizer, test_loader, test_df)\n",
        "\n",
        "print(report)\n",
        "\n",
        "print(report['accuracy'])\n",
        "print(report[0]['f1-score'])\n",
        "print(report[1]['f1-score'])"
      ],
      "metadata": {
        "id": "d2cBAJ3WrI0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_test_dataset[annotated_test_dataset.prediction==1]"
      ],
      "metadata": {
        "id": "xYoGSGZn4LTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_on_intensities(annotated_dataset):\n",
        "    scores = []\n",
        "    for intensity_value in range(0,5):\n",
        "        subset_dataset = annotated_dataset.loc[annotated_dataset.intensity.map(int) == intensity_value]\n",
        "        labels = subset_dataset['label'].tolist()\n",
        "        preds = subset_dataset['prediction'].tolist()\n",
        "        score = f1_score(labels, preds)\n",
        "        print(score)\n",
        "        scores.append(score)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "6H2KXutVbGdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_on_intensities(annotated_test_dataset)"
      ],
      "metadata": {
        "id": "DKPqeZWubXHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_on_input_size(annotated_dataset):\n",
        "    for i in range(0,6):\n",
        "        subset_dataset = annotated_dataset.loc[i*50 <= annotated_dataset.length.map(int) < (i+1)*50]\n",
        "        \n",
        "\n",
        "\n",
        "    plt.title(\"Average Character Length of a sample\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xlabel(\"Character Length\")\n",
        "\n",
        "    # series = df[\"text\"].str.len()\n",
        "\n",
        "    # print(series.max())\n",
        "    # series.hist(bins=200, figsize=(8, 6))\n",
        "\n",
        "    # unpatronising_df[\"text\"].str.len().hist(bins=200, alpha=0.5, figsize=(8, 6))\n",
        "    # patronising_df[\"text\"].str.len().hist(bins=200, alpha=0.5, figsize=(8, 6))\n",
        "\n",
        "    (unpatcounts, unpatbins) = np.histogram(unpatronising_df[\"text\"].str.len(), bins=100, range=(0, 2000))\n",
        "    (patcounts, patbins) = np.histogram(patronising_df[\"text\"].str.len(), bins=100, range=(0, 2000))\n",
        "\n",
        "    factor = 1/len(unpatronising_df[\"text\"])\n",
        "    plt.hist(unpatbins[:-1], unpatbins, weights=factor*unpatcounts, alpha=0.6)\n",
        "\n",
        "    factor = 1/len(patronising_df[\"text\"])\n",
        "    plt.hist(patbins[:-1], patbins, weights=factor*patcounts, alpha=0.6)\n",
        "\n",
        "    plt.xlim((0, 2000))"
      ],
      "metadata": {
        "id": "39M3nT_tgY0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(report['accuracy'])\n",
        "print(report[0]['f1-score'])\n",
        "print(report[1]['f1-score'])"
      ],
      "metadata": {
        "id": "iD-2oo9OwSuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "report = classification_report(tot_labels, preds, target_names=[0,1], output_dict= True)\n"
      ],
      "metadata": {
        "id": "ZXGXYABh0aXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}